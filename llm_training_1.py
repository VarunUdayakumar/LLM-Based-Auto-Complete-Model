# -*- coding: utf-8 -*-
"""LLM_training#1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qBVUoePPNNEPi3e0akW5PiRLB-dIiaNf
"""

pip install -U datasets huggingface_hub fsspec

from datasets import load_dataset

dataset = load_dataset("wikitext","wikitext-2-raw-v1")
train_texts=dataset['train']['text']

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
        tokens = tokenizer(examples["text"],padding="max_length",truncation=True,max_length=128)
        tokens["labels"] = tokens["input_ids"].copy()  # Set labels same as input_ids
        return tokens

tokenize_dataset = dataset.map(tokenize_function,batched=True)

from transformers import GPT2LMHeadModel
model=GPT2LMHeadModel.from_pretrained("gpt2")
model.to(device)
model.resize_token_embeddings(len(tokenizer))

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="results",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    save_steps=500,
    save_total_limit=2,
    logging_dir="logs",
    logging_steps=100
)

trainer=Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenize_dataset["train"]
)

trainer.train()

input_text = "the world is"
inputs = tokenizer(input_text, return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_new_tokens=50)
print("Generated:", tokenizer.decode(outputs[0], skip_special_tokens=True))

model.save_pretrained("gpt2-custom")
tokenizer.save_pretrained("gpt2-custom")